---
title: 'Machine Learning: Introduction'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output: ioslides_presentation
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation
set.seed(1337) # To have a seed defined for reproducability

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     fig.align="center"
                     )

### Install packages if necessary
if (!require("pacman")) install.packages("pacman") # package for loading and checking packages :)

### Install and oad packages if necessary
pacman::p_load(tidyverse, magrittr, 
               tidymodels
               )
```


# This session 

Welcome all to this introduction to machine learning (ML). In this session we will:

1. Introduce to the general logic of machine learning
2. How to generalize?
3. The Bias-Variance Tradeoff
4. Selecting and tuning ML models

# Introduction 

## What is ML?

As with any concept, machine learning may have a slightly different definition, depending on whom you ask. A little compilation of definitions by academics and practioneers alike:

* "Machine Learning at its most basic is the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world." - Nvidia 
* "Machine learning is the science of getting computers to act without being explicitly programmed." - Stanford
* "Machine learning is based on algorithms that can learn from data without relying on rules-based programming."- McKinsey & Co.
* "Machine learning algorithms can figure out how to perform important tasks by generalizing from examples." - University of Washington
* "The field of Machine Learning seeks to answer the question "How can we build computer systems that automatically improve with experience, and what are the fundamental laws that govern all learning processes?" - Carnegie Mellon University

## Supervised vs. Unsupervised ML {.smaller}

### Unsupervised ML

Tasks related to pattern recognition and data exploration, in dase there yet does not exist a right answer or problem structure. Main application

1. **Dimensionality reduction:** Finding patterns in the features of the data
2. **Clustering:** Finding homogenous subgroups within larger group

### Supervised ML

* Concerned with labeling/classification/input-output-mapping/prediction tasks
* Subject of the next lecture, so stay patient

This is what is currently driving >90% ML applications in research, industry, and policy, and will be the focus on the following sessions.

## Supervised vs. Unsupervised ML
### An intuitive perspective

![](https://www.dropbox.com/s/45m8ef7qsmqbhvs/ml_super_unsuper2.png?dl=1){width=500x}

### A functional perspective

![](https://www.dropbox.com/s/yeb7tnyo7vkij42/ml_super_unsuper.png?dl=1){width=500x}


# Contrasting ML with inferential statistics

## Inferential Statistics {.smaller}
* Mostly interested in producing good **parameter estimates**: Construct models with unbiased estimates of $\beta$, capturing the relationship  $x$ and $y$.
* Supposedly \enquote{structural} models: Causal effect of directionality $x \rightarrow y$, robust across a variety of observed as well as up to now unobserved settings.
* How: Carefully draw from  theories and empirical findings, apply logical reasoning to formulate hypotheses.
* Typically, multivariate testing, cetris paribus.
* Main concern: Minimize standard errors $\epsilon$ of $\beta$ estimates.
* Not overly concerned with overall predictive power (eg. $R^2$) of those models, but about various type of endogeneity issues, leading us to develop sophisticated **identification strategies**

## ML Approach {.smaller}
* To large extend driven by the needs of the private sector $\rightarrow$ data analysis is gear towards producing good **predictions** of outcomes $\rightarrow$ fits for $\hat{y}$, not $\hat{\beta}$
     * Recommender systems: Amazon, Netflix, Sportify ect.
     * Risk scores}: Eg.g likelihood that a particular person has an accident, turns sick, or defaults on their credit.
     * Image classification: Finding Cats & Dogs online
     * Predictive policing
* Often rely on big data (N,$x_i$)
* Not overly concerned with the properties of parameter estimates, but very rigorous in optimizing the overall prediction accuracy.
* Often more flexibility wrt. the functional form, and non-parametric approaches.
* No "build-in"" causality guarantee $\rightarrow$ verification techniques.
* Often sporadically used in econometric procedures, but seen as "son of a lesser god". 


# Statistics Refresher

## Introduction to regression problems {.smaller}

Lets for a second recap linear regression techniques, foremost the common allrounder and workhorse of statistical research since some 100 years.

### OLS Basic Properties

* Outcome: contionous 
* Predictors: continous, dichotonomous, categorical
* When to use: Predicting a phenomenon that scales and can be measured continuously

### Functional form

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon $$

 where: 
 
* $y$ = Outcome, $x_i$ = observed value $ID_i$ 
* $\beta_0$ = Constant 
* $\beta_i$ = Estimated effect of $x_i$  on $y$ , slope of the linear function 
* $\epsilon$ = Error term 

--- 

And that is what happens. Lets imagine we plot some feature against an outcome we want to predict. OLS will just fit a straight line through your data.

![](https://www.dropbox.com/s/3v5qka4630kqq6m/reg1.png?dl=1){width=250px}

We do so by minimizing the sum of (squared) errors between our prediction-line and the observed outcome.

![](https://www.dropbox.com/s/1uqge38zhj12sxn/reg2.png?dl=1){width=250px}

## Regression Example

* Let' do a brief example for a simple linear model. 
* We generate some data, where $y$ is a linear function of $x$ plus some random error.

```{r}
data <- tibble(x = runif(500, min = 0, max = 100), 
               y = 15 + (x*0.3) + rnorm(500, sd = 5))
```

```{r,echo=FALSE, fig.width=5,fig.height=2.5}
data %>% ggplot(aes(x = x, y = y)) + 
  geom_point() +
  geom_rug(size = 0.1, alpha = 0.75) 
```

---

We can now fit a linear regression model that aims at discovering the underlying relationship.

```{r}
fit_lm <- data %>% glm(formula = y ~ x, family = gaussian)
fit_lm %>% summary()
```

---

* We can also visualize that
* We see the model puts a straight line through our data
* This line tells us how to predict the outcome `y` based on observed values of `x` 
* The coeffficient indicates the slope of this linear function.

```{r,echo=FALSE, fig.width=5,fig.height=2.5}
data %>% ggplot(aes(x = x, y = y)) + 
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = TRUE)
```

## Prediction based on fitted model

* After the model's parameters are fitted, we can use it to pedict our outcome of interest. 
* Is here done on the same data, but obviously in most cases more elevant on new data.

```{r}
pred_lm <- fit_lm %>% predict() 
pred_lm %>% head()
```

## Assesing predictive power of the model

* So, how well does our model now predict?
* A common measure of predictive power of regressions models is the *Root-Mean-Squared-Error* (RSME), calculate as follows:

$$ RMSE = \sqrt{\frac{1}{n}\Sigma_{i=1}^{n}{\Big(y_i - \hat{y_i} \Big)^2}}$$

Keep in mind, this root&squared thingy does nothing with the error term except of transforming negative to positive values.

```{r}
error <-  pull(data, y) -  pred_lm

sqrt(mean(error ^ 2)) # Calculate RMSE
```

---

* We can also visualize the error term
* Appears to be rather normally distributed.

```{r}
error %>% as_tibble() %>%
  ggplot(aes(x = value)) + 
  geom_histogram() 
```


## Introduction to classification problems

* Lets assume our outcome of interest is categorigal (Yes/No, Class1/Class2/Class3...)
* 


```{r}
data <- tibble(
  x = rnorm(500),
  y = rbinom(500, size = 1, prob = 1/(1+exp(-(5*x))) ) 
  )
data %>% head()
```

---

* This is how it looks like.

```{r,echo=FALSE}
data %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = 0.5)
```

---

* We can obviously fit a linear model on it. 
* What do the predicted values mean then? 
* We could intrpet them as `probability: y=TRUE`
* However, how does the model fit the data?

```{r,echo=FALSE}
data %>% ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", formula = y ~ x, se = TRUE)
```

--- 

* I gues we would like more to have something like this below, right?
* This seems to be more suited for class prediction, right?

```{r,echo=FALSE}
data %>% ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = 0.5) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) 
```

---

* How do we do that?
* The easiest way is to use a `glm`, where we just change the distribution

```{r}
fit_log <- data %>%
  glm(formula = y ~ x, family = binomial)

fit_log %>% summary
```

---

* based on this model we can now also carry out predictions

```{r}
pred_log <- fit_log %>% predict(type = "response") 
pred_log %>% head()
```



## Model assesments and metrics for classification problems {.smaller}

![](https://www.dropbox.com/s/dn1juxbdfm8ryi8/cf1.jpg?dl=1){width=500px}

* We remeber that the most commonly used performance measure for regression problems is the **RMSE**. 
* However, how to we assess models aimed to solve classification problems? Here, it is not that straightforward, and we could (depending on the task) use different ones.
* Most measures of predictive performance in classification tasks are derived from the confusion matrix.
* The **Confusion Matrix** (in inferential statistics you would call it **Classification Table**, so don't get confused) is the main source 


## Confussion Matrix Unpacked {.smaller}

It is the 2x2 matrix with the foillowing cells:

* **True Positive:** (TP)
     * Interpretation: You predicted positive and it's true.
     * You predicted that a woman is pregnant and she actually is.
* **True Negative:** (TN)
     * Interpretation: You predicted negative and it's true.
     * You predicted that a man is not pregnant and he actually is not.
* **False Positive:** (FP) - (Type 1 Error)
     * Interpretation: You predicted positive and it's false.
     * You predicted that a man is pregnant but he actually is not.
* **False Negative:** (FN) - (Type 2 Error)
     * Interpretation: You predicted negative and it's false.
     * You predicted that a woman is not pregnant but she actually is.

* Just remember, We describe predicted values as **Positive** and **Negative** and actual values as **True** and **False**. 
* Out of combinations of these values, we dan derive a set of different quality measures.

## Summary of Metrics {.smaller}

**Accuracy** (ACC)
$$ {ACC} ={\frac {\mathrm {TP} + \mathrm {TN} }{P+N}} $$

**Sensitivity** also called recall, hit rate, or true positive rate (TPR)
$$ {TPR} ={\frac {\mathrm {TP} }{P}}={\frac {\mathrm {TP} }{\mathrm {TP} +\mathrm {FN} }}$$

**Specificity**, also called selectivity or true negative rate (TNR)
$$ {TNR} ={\frac {\mathrm {TN} }{N}}={\frac {\mathrm {TN} }{\mathrm {TN} +\mathrm {FP} }}$$ 

**Precision**, also called positive predictive value (PPV)
$$ {PPV} ={\frac {\mathrm {TP} }{\mathrm {TP} +\mathrm {FP} }} $$ 

**F1 score**: weighted average of the true positive rate (recall) and precision.
$$ F_{1}={\frac {2\mathrm {TP} }{2\mathrm {TP} +\mathrm {FP} +\mathrm {FN} }} $$

## Creating a confusion matrix

* We can create it on our own
* First, we create a tibble with the real and predicted values side-by-side

```{r}
res_log <- tibble(
  y = data %>% pull(y) %>% as.factor(),
  y_pred_prob = pred_log,
  y_pred = pred_log %>% round(0) %>% as.factor(),)
```

* Then, we can create the confussion matrix

```{r}
cm_log <- res_log %>% conf_mat(y, y_pred)
```

---

```{r}
cm_log %>% autoplot(type = "heatmap")
```

---

```{r}
cm_log %>% summary()
```

## ROC and AUC
* An ROC curve (receiver operating characteristic curve, weird name, i know. Comes originally from signal processing) is a derivative of the confusion matrix and predicted class-probabilities.
* It tells us how sensitive our classification is to 
* The area-under-the-curve (AUC) gies us another good indicator of a models predictive power

```{r}
roc_log <- res_log %>% roc_curve(y, y_pred_prob)
roc_log %>% head()
```

---

```{r}
roc_log %>% autoplot()
```

# Generalization in ML models

## Generalization via "Out-of-Sample-Testing" {.smaller}

With so much freedom wrt. feature selection, functional form ect., models are prone to over-fitting. And no constraints by asymptotic properties, causality and so forth, how can we generalize anything?

In ML, generalization is not achived by statistical derivatives and theoretical argumentation, but rather by answering the practical question: **How well would my model perform with new data?** To answer this question, **Out-of-Sample-Testing** is usually taken as solution. Here, you do the following

1. Split the dataset in a training and a test sample.
2. Fit you regression (train your model) on one dataset
     * Optimal: Tune hyperparameters by minimizing loss in a validation set. 
     * Optimal: Retrain final model configuration on whole training set
3. Finally, evaluate predictive power on test sample, on which model is not fitted.
			
An advanced version is a **N-fold-Crossvalidation**, where this process is repeated several time during the **hyperparameter-tuning** phase (more on that later).

---

![](https://www.dropbox.com/s/mckm524jdserm2x/cv_steps.png?dl=1){width=700px}

## Bias-Variance Tradeoff

![](https://www.dropbox.com/s/66o5gvtaeh6n5ut/learningmodels.png?dl=1){width=500px}

* As a rule-of-thumb: Richer and more complex functional forms and algorithms tend to be better in predictign complex real world pattern. This is particularly true for high-dimensional (big) data.
* However, flexible algorithms at one point become so good in mimicing the pattern in our data that they **overfit**, meaning are to much tuned towards a specific dataset and might not reproduce the same accuracy in new data. 

## Bias-Variance Tradeoff {.smaller}

Generally, we call this tension the **bias-variance tradeoff**, which we can decompose in the two components:

1. **Bias Error** The simplifying assumptions made by a model to make the target function easier to learn. Generally, simple parametric algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. 
2. **Variance Error:** Variance is the amount that the estimate of the target function will change if different data was used. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables. 

$${\displaystyle \operatorname {E} {\Big [}{\big (}y-{\hat {f}}(x){\big )}^{2}{\Big ]}={\Big (}\operatorname {Bias} {\big [}{\hat {f}}(x){\big ]}{\Big )}^{2}+\operatorname {Var} {\big [}{\hat {f}}(x){\big ]}+\sigma ^{2}}$$

* As a result, the predictive performance (on new data) of algorithms and models will always depend on this trade-off between bias and variance. Mathematically, this can be formalized as:
* Note that why bias and variance are reducible errors which decline when using a more suitable model to model the underlying relationships in the data, $\sigma^2$ denotes the unreducible complexity caused by random noise, measurement errors, or missing variables, which represent a boundary to our ability to predict given the data at hand.

## Example {.smaller}

* We create some data, where $x$ is a uniformly distributed random variable bounded between 0-1, and $y = sin(n)$ 

```{r}
data <- tibble(x = runif(50, min = 0, max = 3.14),
               y = sin(x) )
```

```{r,echo=FALSE}
data %>% ggplot(aes(x = x, y = y)) + 
  geom_point()
```

---

How, we add some random noise, which is normally distributed

```{r}
error <- rnorm(n = 50, mean = 0, sd = 0.05)
data %<>% mutate(y_e = y + error)
```

```{r,echo=FALSE}
data %>% ggplot(aes(x = x, y = y_e)) + 
  geom_point()
```

## Fitting different models 

We see the formerly clearly visible underlying relationship between $x$ and $y$ now to some extent disturbed by this noise. However, keep in mind that the process that generated the data is still $y = sinus(x)$, which would also be the best funtional form to identify by any predictive algorithm.

Lets see how models with different levels of complexity would interpret the reÃ¦lationship between $x$ and $y$:


---

  2. $y$ is modeled as a linear function of $x$
  3. $y$ is modeled as a curvelinear function of $x$
  3. $y$ is modeled as a compex multinomial function of $x$

```{r,echo=FALSE}
data %>% ggplot(aes(x = x, y = y_e)) + 
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, 
              se = FALSE, aes(colour = "linear"), linetype = "dashed")  +
  geom_smooth(method = "lm", formula = y ~ x + poly(x, 2), 
              se = FALSE, aes(colour = "curvelinear"), linetype = "dotted") +
  geom_smooth(method = "lm", formula = y ~ x + poly(x, 20), 
              se = FALSE, aes(colour = "polynomial"), linetype = "longdash")      
```

In this case, we clearly se that a linear model is too simple, therefore introduces a lot of bias. The curvelinear model indead approximates the underlying relationship correct, while the curvelinear model indeed presents a better fit to the data at hand but misses the underlying relationship. It most likely would perform worse on new data subject to the same random variation.

---

![](https://www.dropbox.com/s/yai8hbcvbo3haoq/ml_bias_variance1.jpg?dl=1){width=750px}

## Regularization {.smaller}

![https://www.dropbox.com/s/3svghvjeukabrwp/ml_complexity_error.png?dl=1(){width=500px}

*  The process of minimizing bias and variance errors is called **regularization** (inpractice also refered to as **hyperparameter-tuning**)
* We aim at selecting the right model class, functional form, and degree of complexity to jointly minimize in-sample loss but also between-sample variations.


---

Mathematically speaking, we try to minimize a loss function $L(.)$ (eg. RMSE) the following problem:

$$minimize \underbrace{\sum_{i=1}^{n}L(f(x_i),y_i),}_{in-sample~loss} ~ over \overbrace{~ f \in F ~}^{function~class} subject~to \underbrace{~ R(f) \leq c.}_{complexity~restriction}$$

## Hyperparameter Tuning

* Most model classes have parameters influencing their functionality
* These parameters often influence predictive performance of models
* Finding the right hyperparameter configuration (tuning) is therefore an essiential part when engineering predictive algorithms

---

![](https://www.dropbox.com/s/atva3mgrn3kpy8r/ml_hyperparam1.png?dl=1){width=750px}

---

## Crossvalidation {.smaller}

1. Split the dataset in a training and a test sample.
2. Fit you regression (train your model) on one dataset
     * Optimal: Tune hyperparameters by minimizing loss in a validation set. 
     * Optimal: Retrain final model configuration on whole training set
3. Finally, evaluate predictive power on test sample, on which model is not fitted.
			
An advanced version is a **N-fold-Crossvalidation**, where this process is repeated several time during the **hyperparameter-tuning** phase (more on that later).

---

![](https://www.dropbox.com/s/mckm524jdserm2x/cv_steps.png?dl=1){width=750px}

## Tune Grids

![](https://www.dropbox.com/s/dogot9mi70y5ueq/ml_hyperparam2.png?dl=1){width=500px}

---

![](https://www.dropbox.com/s/z5v50gsef2vfpwm/ml_hyperparam3.png?dl=1){width=750px}


# Examples of Model classes

## Elastic Net
The elastic net has the functional form of a generalized linear model, plus an adittional term $\lambda$ a parameter which penalizes the coefficient by its contribution to the models loss in the form of:

$$\lambda \sum_{p=1}^{P} [ 1 - \alpha |\beta_p| + \alpha |\beta_p|^2]$$

* Here, we have 2 tunable parameters, $\lambda$ and $\alpha$.  If $\alpha = 0$, we are left with $|\beta_i|$, turning it to a lately among econometricians very popular **Least Absolute Shrinkage and Selection Operator** (LASSO) regression. 
* Obviously, when $\lambda = 0$, the whole term vanishes, and we are again left with a generalized linear model. 


## Decision Trees

What this interesting family of models is about:

* Mostly used in classification problems on continuous or categorical variables.
* Idea: split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables.
* Repeat till stop criterium reachesd. leads to a tree-like structure.

![](https://www.dropbox.com/s/rhdx8upcikkun7p/regtree0.png?dl=1){width=500px}

--- 

This class became increasingly popular in business and other applications. Some reasons are:

* Easy to Understand: Decision tree output is very easy to understand even for people from non-analytical background.
* Useful in Data exploration: Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables.
* Data type is not a constraint: It can handle both numerical and categorical variables.
* Non Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.

## Some tree terminology {.smaller}

* **Root Node:** Entire population or sample and this further gets divided into two or more homogeneous sets.
* **Splitting:** It is a process of dividing a node into two or more sub-nodes.
* **Decision Node:** When a sub-node splits into further sub-nodes, then it is called decision node.
* **Leaf/ Terminal Node:** Nodes do not split is called Leaf or Terminal node.

![](https://www.dropbox.com/s/j5ise9gi226rsnn/regtree2.png?dl=1){width=500px}

---

The decision of making strategic splits heavily affects a tree's accuracy. So, How does the tree decide to split? This is different across the large family of tree-like models. Common approaches:

* Gini Index
* $\chi^2$ 
* Reduction in $\sigma^2$

Some common complexity restrictions are:

* Minimum samples for a node split
* Minimum samples for a terminal node (leaf)
* Maximum depth of tree (vertical depth)
* Maximum number of terminal nodes
* Maximum features to consider for split	

---

Likewise, there are a veriety of tunable hyperparameters across different applications of this model family.

![](https://www.dropbox.com/s/dwl89havzpl1qla/regtree3.png?dl=1){width=750px}

## Random Forest

* As a continuation of tree-based classification methods, random forests aim at reducing overfitting by introducing randomness via bootstrapping, boosting, and ensemble techniques. 
* It is a type of ensemble learning method, where a group of weak models combine to form a powerful model. 
* The idea here is to create an "ensemble of classification trees"", all grown out of a different bootstrap sample. Having grown a forest of trees, every tree performs a prediction, and the final model prediction is formed by a majority vote of all trees. 
* This idea close to Monte Carlo simulation approaches, tapping in the power of randomness.

--- 

![](https://www.dropbox.com/s/f0a2afec92awyw0/rf2.png?dl=1){width=750px}

# Summing up

## What we covered so far

In this session, took a look at 

* What is ML to start with?
* The difference between traditional inferential statistics and ML
* Out-of-Sample validation
* The Bias-Variance Tradeoff
* How to tune ML models
* ML model classes

In the next session, we will apply what we learned so far... so stay tuned!
