---
title: 'Machine Learning: Applications in technology Analysis 1'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation
set.seed(1337) # To have a seed defined for reproducability

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     fig.align="center"
                     )

### Install packages if necessary
if (!require("pacman")) install.packages("pacman") # package for loading and checking packages :)
```

```{r}
### Install and oad packages if necessary
pacman::p_load(tidyverse, 
               magrittr, 
               tidymodels
               )
```


Welcome to todays session. 

# Exploring Chinese Patent Data

## Introduction

* So, let's start the fun. 
* I for you extracted Chinese patents from our EPO [PATSTAT](https://www.epo.org/searching-for-patents/business/patstat.html) databases filed at either the EPO or the USTPO. 
* I further provide you adittional data to be found in PATSTAT. 



```{r}
patents <- readRDS(url("https://github.com/daniel-hain/ML_course_maastricht/raw/master/data/CN_patent.rds?raw=true"))
pat_abstr <- readRDS(url("https://github.com/daniel-hain/ML_course_maastricht/raw/master/data/CN_el_patent_abstract.rds?raw=true"))
pat_cpc <- readRDS(url("https://github.com/daniel-hain/ML_course_maastricht/raw/master/data/CN_el_cpc.rds?raw=true"))
```

* Filter for post-2013

```{r}
patents %<>% filter(appln_filing_year >= 2013)
```

* Filter for only patents whith entry in main data table

```{r}
pat_abstr %<>% semi_join(patents, by = 'appln_id')
pat_cpc %<>% semi_join(patents, by = 'appln_id')
```


## Patent main data

```{r}
patents %>% head()
```

```{r}
patents %>% glimpse()
```

This main dataset contains all Patents in the 2000-2015 period with Chinese inventors, filed at the USTPO or EPO. I only included priority (earliest) patent applications which got granted up to now. We have the following variables:

* `appln_id`: PATSTAT id, unique identifier of patent application
* `appln_filing_year`: Filing year of first priority
* `docdb_family_size`: Size of the (simple) patent family
* `nb_citing_docdb_fam`: Number of citations recieved by the patent family
* `nb_inventors`: Number of inventors

```{r}
patents %>% skimr::skim()
```

## Patent CPC Class

```{r}
pat_cpc %>% glimpse()
```

```{r}
pat_cpc %>% head()
```

```{r}
pat_cpc %>% 
  count(cpc_class_symbol, sort = TRUE) %>%
  head(10)
```

## Patent Abstracts

```{r}
pat_abstr %>% glimpse()
```

```{r}
pat_abstr %>% select(-appln_id) %>% head()
```

# Exploratory Analysis

We could have so much fun here exploring Chinese patents, but we have no time. However, I do another more exploratory lecture on the ame dataset, feel free to check:

* [Economic Geography & Patents](https://daniel-hain.github.io/SDC_IM/notebooks/S3_1_Economic_geography.html)
* [Economic Complexity & Patents](https://daniel-hain.github.io/SDC_IM/notebooks/S3_2_Economic_complexity.html)

# The Y Tag: How to identify renewable energy patents

We now aim at identifying renewable energy patents. This could be the starting point for an interesting analysis on all kind of things, but we here went to ask the following question: 

1. Could we develop a model that detects renewable energy patents based on their abstract?

* We exploit the WIPOs Y-tag. 
* Here, the WIPO labels patents identified to be related to renewable energy with the adittional CPC class assignment `Y02` which helps us to easily identify them. 
* Check [here](https://www.gonst.lu.se/article/report-on-green-patents) for further information.

Lets identify renewable energy patents.

```{r}
y_tag <- pat_cpc %>%
  filter(cpc_class_symbol %>% str_starts('Y02')) %>%
  distinct(appln_id) %>%
  pull()
```

```{r}
patents %<>%
  mutate(y_tag = appln_id %in% y_tag)
```

```{r}
patents %>% head()
```

```{r}
rm(pat_cpc)
```


# Text analysis of patent data

## The R NLP ecosystem (Brief reminder)

Most language analysis approaches are based on the analysis of texts word-by-word. Here, their order might matter (word sequence models) or not (bag-of-words models), but the smallest unit of analysis is usually the word. This is usually done in context of the document the word appeared in. Therefore, on first glance three types datastructures make sense:

1. **Tidy:**  Approach, where data is served in a 2-column document-word format (e.g., `tidytext`)
2. **Token lists:** Creation of special objects, saved as document-token lists or corpus (e.g., `tm`, `quanteda`)
3. **Matrix:** Long approach, where data is served as document-term matrix, term-frequency matrix, etc.

Different forms of analysis (and the packages used therefore) favor different structures, so we need to be fluent in transfering original raw-text in these formats, as well as switching between them. (for more infos, check [here](https://www.tidytextmining.com/dtm.html)).

![](https://www.dropbox.com/s/ou05c8np4j47r0q/nlp_tidyworkflow.png?dl=1)

```{r}
library(tidytext)
```

## Tidy Text Formats

```{r}
pat_abstr %<>%
  left_join(patents %>% select(appln_id, y_tag), by = "appln_id")
```


```{r}
pat_abstr_tidy <- pat_abstr %>%
  select(appln_id, y_tag, appln_abstract) %>%
  unnest_tokens(output = word, 
                input = appln_abstract, 
                token = "words",
                to_lower = TRUE,
                drop = TRUE)
```


```{r}
pat_abstr_tidy %>%
  head()
```

```{r}
 pat_abstr_tidy %<>%
  mutate(word = word %>% str_remove_all('[^[:alnum:]]')) %>%
  filter(str_length(word) > 2 ) %>%
  group_by(word) %>%
  filter(n() > 100) %>%
  ungroup() %>%
  anti_join(get_stopwords()) 
```

```{r}
pat_abstr_tidy %<>%
  add_count(appln_id, word) %>%
  bind_tf_idf(term = word,
              document = appln_id,
              n = n)
```

```{r}
pat_abstr_tidy %>%
  head()
```

```{r}
pat_abstr_tidy %>%
  count(word, wt = tf_idf) %>%
  top_n(n, 20)
```

```{r}
pat_ytag_words <- pat_abstr_tidy %>%
  group_by(y_tag) %>%
  count(word, wt = tf_idf, sort = TRUE, name = "tf_idf") %>%
  slice(1:20) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, by = tf_idf, within = y_tag)) 
```

```{r}
pat_ytag_words %>%
  ggplot(aes(x = word, y = tf_idf, fill = y_tag)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~y_tag, ncol = 2, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```
## Towards a predictive model?

* We see there seems to be quite a difference in the words used in Y-tagged renewable energy patents. 
* Therefore it indeed might be possible to build a predictive model based on text...
* How would we do that?
* The easiest way would be to create a document-term-matrix, basically creating a dummy variable for every term.
* We could do it a bit more sophisticated, and instead use the TFIDF weight.

```{r}
# only a small sample for illustration
pat_dtm <- pat_abstr_tidy %>%  
  select(appln_id, y_tag, word, tf_idf) %>%
  head(1000) %>%
  distinct(appln_id, word, .keep_all = TRUE)
```

```{r}
# Now we just pivot wider
pat_dtm %<>% 
  pivot_wider(names_from = word, values_from = tf_idf, names_prefix = 'word_', values_fill = 0)

```

```{r}
pat_dtm %>% 
  head()
```

* However, there are easier ways to do so, if the purpose is to create a predictive model.

# Building a predictive model

```{r}
data <- pat_abstr %>%
  select(y_tag, appln_abstract) %>%
  rename(y = y_tag, text = appln_abstract) %>%
  mutate(y = y %>% as_factor()) %>%
  mutate(text = text %>% str_to_lower() %>% str_remove_all('[^[:alnum:] ]'))
```


```{r}
data %>% skimr::skim()
```



## Training & Test split

```{r}
data_split <- initial_split(data, prop = 0.75, strata = y)

data_train <- data_split  %>%  training()
data_test <- data_split %>% testing()
```

## Preprocessing pipeline

* While we up to now did all the NLP by hand, we can also just use the textrecipes package to automatize the workflow

```{r}
library(textrecipes)
```

```{r}
data_recipe <- data_train %>%
  recipe(y ~.) %>%
  step_downsample(y) %>% # due to class imbalances
  step_filter(text != "") %>% # remove 
  step_tokenize(text) %>% # Tokenizing
  step_tokenfilter(text, min_times = 100) %>%  # filter sparse terms
  step_stopwords(text, keep = FALSE) %>% # Filter stopwords
  step_tfidf(text) %>% # tfidf weighting
  prep()
```

```{r}
data_recipe
```

```{r}
data_train_prep <- data_recipe %>% juice()
data_test_prep <- data_recipe %>% bake(data_test)
```


## Defining the models



```{r}
# library(doParallel)
# all_cores <- parallel::detectCores(logical = FALSE)

# cl <- makePSOCKcluster(all_cores -1)
#registerDoParallel(cl)
```


```{r}
model_en <- logistic_reg(mode = 'classification', 
                         mixture = 0.25, 
                         penalty = 0.25) %>%
  set_engine('glm', family = binomial) 
```


## Define the workflow

We will skip the workflow step this time, since we do not evaluate different models against each others.

## fit the model

```{r}
fit_en <- model_en %>% fit(formula = y ~., data = data_train_prep)
```


```{r}
pred_collected <- tibble(
  truth = data_train_prep %>% pull(y),
  pred = fit_en %>% predict(new_data = data_train_prep) %>% pull(.pred_class),
  pred_prob = fit_en %>% predict(new_data = data_train_prep, type = "prob") %>% pull(.pred_TRUE),
  ) 
```

```{r}
pred_collected %>% conf_mat(truth, pred)
```

```{r}
pred_collected %>% conf_mat(truth, pred) %>% summary()
```


# Model explainability
Machine learning (ML) models are often considered black boxes due to their complex inner-workings. More advanced ML models such as random forests, gradient boosting machines (GBM), artificial neural networks (ANN), among others are typically more accurate for predicting nonlinear, faint, or rare phenomena. Unfortunately, more accuracy often comes at the expense of interpretability, and interpretability is crucial for business adoption, model documentation, regulatory oversight, and human acceptance and trust. Luckily, several advancements have been made to aid in interpreting ML models.

![](https://www.dropbox.com/s/r5w3o80q3k3prdm/interpret.png?dl=1){width=750px}

Moreover, it's often important to understand the ML model that you've trained on a global scale, and also to zoom into local regions of your data or your predictions and derive local explanations. 

* **Global interpretations** help us understand the inputs and their entire modeled relationship with the prediction target, but global interpretations can be highly approximate in some cases. 
* **Local interpretation** help us understand model predictions for a single row of data or a group of similar rows.


## Global explanation
Finally, let's get a feeling what variables the models mostly draw from. There are numerous ways for such inspections, in which we will dewll deeper in later lectures. Here, I just want to present the most intuitive and common one, **Variable Importance**.

Most (but not all) model classes offer some possibility to derive measures of variable importance. Note, this is currently not implemented for SVMs. Again, in most ML models and setups, these meastures are nice to give a rough intuition, but CANNOT be interpreted as constant marginal effects, left alone causal.

```{r}
library(vip)
```

```{r}
vip(fit_en) + ggtitle("VarImp: Elastic Net")
```

## Local interpretations
In complex nonparametric models, it is often more helpful to get explanations why a certain datapoint is classified in the way it is than to look at the overall importance of variables. 

**Local Interpretable Model-agnostic Explanations** (LIME) is a visualization technique that helps explain individual predictions. As the name implies, it is model agnostic so it can be applied to any supervised regression or classification model. There is a neath `R` implementation in the [`lime`](https://lime.data-imaginist.com) package. If you want to investigate further, feel free! Otherwise, keep in mind this exists, we will come back to it again in later lectures.

BTW: The original paper is mindblowing, if you find time, just read it!

* Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. ""hy Should I Trust You?: Explaining the Predictions of Any Classifier." In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2016). ACM, New York, NY, USA, 1135-1144. DOI: https://doi.org/10.1145/2939672.2939778
```{r}
library(lime)
```

```{r}
# Create an explainer object
explainer <- lime(data_train_prep, fit_en)
```

```{r}
# Explain new observation
explanation <- explain(data_train_prep %>% slice(1:6), explainer, n_labels = 1, n_features = 5)
```

```{r}
# The output is provided in a consistent tabular format and includes the output from the model.
explanation %>% head()
```

```{r, fig.height=10, fig.width=15}
explanation %>% plot_features()
```



### Bonus: Own predictions

* We could also just apply the model to some own text and see how it predicts...

```{r}
text_own = tibble(text = 'This device is going to save the world that includes a portion of solar photovoltaic power to control bio enriched energy saviung algae that produces a highly energy body state that can be used for mobile energy production.')
```

* We run this text through our preprocessing recipe and let our model predict...

```{r}
fit_en %>% predict(new_data = data_recipe %>% bake(text_own))
```

```{r}
text_cordis <- read_csv('https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/cordis-h2020reports.gz') 
```

```{r}
pred_cordis <- tibble(pred = fit_en %>% predict(new_data = data_recipe %>% bake(text_cordis %>% rename(text = summary) %>% select(text))) %>% pull(.pred_class),
                      pred_prob = fit_en %>% predict(new_data = data_recipe %>% bake(text_cordis %>% rename(text = summary) %>% select(text)), type = "prob") %>% pull(.pred_TRUE))

```
```{r}
text_cordis %>%
  bind_cols(pred_cordis) %>%
  arrange(desc(pred_prob)) %>%
  select(projectAcronym, title, pred) %>%
  slice(1:2)
```




# Endnotes

### References

* [Hain, D., & Jurowetzki, R. (2020). Introduction to Rare-Event Predictive Modeling for Inferential Statisticians--A Hands-On Application in the Prediction of Breakthrough Patents. arXiv preprint arXiv:2003.13441.](https://arxiv.org/abs/2003.13441)

### Packages and Ecosystem

* [`tidymodels`](https://www.tidymodels.org/): Tidy statistical and predictive modeling ecosystem
* [`tidytext`](https://juliasilge.github.io/tidytext/): Tidy text analysis in R ecosystem
* [`textrecipes`](https://textrecipes.tidymodels.org/): Preprocessing workflows for text data



### Further Readings

* [`Tidy Text Mining in R`](https://www.tidytextmining.com/)

### Session info

```{r}
sessionInfo()
```
